{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48086d92-d72b-41e2-9fb3-369e1a2d9974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Concatenate, LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "214bda5f-8429-4467-8c23-b490df0ab247",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions = pd.read_json('data/transactions.json', lines=True)\n",
    "df_users = pd.read_csv('data/users.csv')\n",
    "df_merchants = pd.read_csv('data/merchants.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2e91ad5-4f60-479d-8ab1-f6e783f8f391",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.merge(df_transactions, df_users, on='user_id', how='left')\n",
    "\n",
    "# Then, join the result with df_merchants on merchant_id\n",
    "df_merged = pd.merge(df_merged, df_merchants, on='merchant_id', how='left')\n",
    "\n",
    "df = df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34ff4921-d8fe-49b7-8670-cc4ed8c1ceaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [\n",
    "    'channel', 'currency', 'device', 'payment_method', 'category',\n",
    "    'country_x', 'country_y', 'sex', 'education', 'primary_source_of_income'\n",
    "]\n",
    "\n",
    "binary_cols = [\n",
    "    'is_international', 'is_first_time_merchant', 'has_fraud_history'\n",
    "]\n",
    "\n",
    "numerical_cols = [\n",
    "    'amount', 'session_length_seconds', 'age', 'risk_score', 'trust_score',\n",
    "    'number_of_alerts_last_6_months', 'avg_transaction_amount',\n",
    "    'account_age_months', 'sum_of_monthly_expenses', 'sum_of_monthly_installments'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b23bab8c-4874-41c5-bed8-b43c0147fa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df['signup_date'] = pd.to_datetime(df['signup_date'])\n",
    "\n",
    "# Temporal features\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "df['days_since_signup'] = (df['timestamp'] - df['signup_date']).dt.total_seconds() / (3600 * 24)\n",
    "\n",
    "numerical_cols += ['hour', 'day_of_week', 'days_since_signup']\n",
    "\n",
    "# Location\n",
    "df['lat'] = df['location'].apply(lambda x: x['lat'] if isinstance(x, dict) else np.nan)\n",
    "df['long'] = df['location'].apply(lambda x: x['long'] if isinstance(x, dict) else np.nan)\n",
    "\n",
    "numerical_cols += ['lat', 'long']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a603b00d-dd28-41cf-9041-bbb19cb11fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process categorical variables\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Process binary variables\n",
    "for col in binary_cols:\n",
    "    df[col] = df[col].astype(int)\n",
    "\n",
    "# Normalize numerical features\n",
    "scaler = MinMaxScaler()\n",
    "df[numerical_cols] = scaler.fit_transform(df[numerical_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "215fd19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500000 entries, 0 to 499999\n",
      "Data columns (total 35 columns):\n",
      " #   Column                          Non-Null Count   Dtype         \n",
      "---  ------                          --------------   -----         \n",
      " 0   transaction_id                  500000 non-null  object        \n",
      " 1   timestamp                       500000 non-null  datetime64[ns]\n",
      " 2   user_id                         500000 non-null  object        \n",
      " 3   merchant_id                     500000 non-null  object        \n",
      " 4   amount                          500000 non-null  float64       \n",
      " 5   channel                         500000 non-null  int32         \n",
      " 6   currency                        500000 non-null  int32         \n",
      " 7   device                          500000 non-null  int32         \n",
      " 8   location                        500000 non-null  object        \n",
      " 9   payment_method                  500000 non-null  int32         \n",
      " 10  is_international                500000 non-null  int32         \n",
      " 11  session_length_seconds          500000 non-null  float64       \n",
      " 12  is_first_time_merchant          500000 non-null  int32         \n",
      " 13  is_fraud                        500000 non-null  int64         \n",
      " 14  age                             500000 non-null  float64       \n",
      " 15  sex                             500000 non-null  int32         \n",
      " 16  education                       500000 non-null  int32         \n",
      " 17  primary_source_of_income        500000 non-null  int32         \n",
      " 18  sum_of_monthly_installments     500000 non-null  float64       \n",
      " 19  sum_of_monthly_expenses         500000 non-null  float64       \n",
      " 20  country_x                       500000 non-null  int32         \n",
      " 21  signup_date                     500000 non-null  datetime64[ns]\n",
      " 22  risk_score                      500000 non-null  float64       \n",
      " 23  category                        500000 non-null  int32         \n",
      " 24  country_y                       500000 non-null  int32         \n",
      " 25  trust_score                     500000 non-null  float64       \n",
      " 26  number_of_alerts_last_6_months  500000 non-null  float64       \n",
      " 27  avg_transaction_amount          500000 non-null  float64       \n",
      " 28  account_age_months              500000 non-null  float64       \n",
      " 29  has_fraud_history               500000 non-null  int32         \n",
      " 30  hour                            500000 non-null  float64       \n",
      " 31  day_of_week                     500000 non-null  float64       \n",
      " 32  days_since_signup               500000 non-null  float64       \n",
      " 33  lat                             500000 non-null  float64       \n",
      " 34  long                            500000 non-null  float64       \n",
      "dtypes: datetime64[ns](2), float64(15), int32(13), int64(1), object(4)\n",
      "memory usage: 108.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70700f12-6fc4-4040-bb15-a6e42933d702",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Time-Based Sequences\n",
    "target_column = 'is_fraud'\n",
    "user_column = 'user_id'\n",
    "time_window_hours = 24  # Look back window of 24 hours\n",
    "sequence_length = 10    # Max transactions to consider per window\n",
    "padding_value = 0  \n",
    "\n",
    "data_sequences = []\n",
    "labels = []\n",
    "sequence_indices = []  # To track which rows correspond to which sequences\n",
    "\n",
    "# Group by user\n",
    "grouped = df.groupby(user_column)\n",
    "\n",
    "for user_id, user_data in grouped:\n",
    "    # Sort by timestamp\n",
    "    user_data = user_data.sort_values(by='timestamp')\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    user_data_values = user_data[numerical_cols].values\n",
    "    timestamps = user_data['timestamp'].values\n",
    "    \n",
    "    # Convert timestamps to numpy datetime64\n",
    "    if not np.issubdtype(timestamps.dtype, np.datetime64):\n",
    "        timestamps = np.array([np.datetime64(ts) for ts in timestamps])\n",
    "    \n",
    "    for i in range(len(user_data)):\n",
    "        current_time = timestamps[i]\n",
    "        time_threshold = current_time - np.timedelta64(time_window_hours, 'h')\n",
    "        \n",
    "        # Get indices of transactions within last 24 hours\n",
    "        mask = (timestamps >= time_threshold) & (timestamps <= current_time)\n",
    "        window_indices = np.where(mask)[0]\n",
    "        \n",
    "        # Get transactions within window\n",
    "        window_data = user_data_values[window_indices]\n",
    "        \n",
    "        if len(window_data) > 0:\n",
    "            # Pad if fewer than sequence_length transactions\n",
    "            if len(window_data) < sequence_length:\n",
    "                pad_length = sequence_length - len(window_data)\n",
    "                padding = np.full((pad_length, len(numerical_cols)), padding_value)\n",
    "                window_data = np.vstack([padding, window_data])\n",
    "            \n",
    "            # Take most recent sequence_length transactions\n",
    "            window_data = window_data[-sequence_length:]\n",
    "            \n",
    "            data_sequences.append(window_data)\n",
    "            labels.append(user_data[target_column].iloc[i])\n",
    "            sequence_indices.append(user_data.index[i])  # Store the original index\n",
    "\n",
    "# Convert to numpy arrays\n",
    "data_sequences = np.array(data_sequences)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13f58268-e8a6-4882-a5ef-c6bc1ae54561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Transaction-Count-Based Sequences\n",
    "target_column = 'is_fraud'\n",
    "user_column = 'user_id'\n",
    "sequence_length = 7    # Last 7 transactions to consider\n",
    "padding_value = 0  \n",
    "\n",
    "data_sequences = []\n",
    "labels = []\n",
    "sequence_indices = []  # To track which rows correspond to which sequences\n",
    "\n",
    "# Group by user\n",
    "grouped = df.groupby(user_column)\n",
    "\n",
    "for user_id, user_data in grouped:\n",
    "    # Sort by timestamp to ensure chronological order\n",
    "    user_data = user_data.sort_values(by='timestamp')\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    user_data_values = user_data[numerical_cols].values\n",
    "    \n",
    "    for i in range(len(user_data)):\n",
    "        # Get indices of previous transactions (up to sequence_length-1)\n",
    "        start_idx = max(0, i - (sequence_length - 1))\n",
    "        window_indices = range(start_idx, i + 1)\n",
    "        \n",
    "        # Get the sequence of transactions\n",
    "        window_data = user_data_values[window_indices]\n",
    "        \n",
    "        # Pad if fewer than sequence_length transactions\n",
    "        if len(window_data) < sequence_length:\n",
    "            pad_length = sequence_length - len(window_data)\n",
    "            padding = np.full((pad_length, len(numerical_cols)), padding_value)\n",
    "            window_data = np.vstack([padding, window_data])\n",
    "        \n",
    "        data_sequences.append(window_data)\n",
    "        labels.append(user_data[target_column].iloc[i])\n",
    "        sequence_indices.append(user_data.index[i])  # Store the original index\n",
    "\n",
    "# Convert to numpy arrays\n",
    "data_sequences = np.array(data_sequences)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2d83e5c-e6b9-4a90-a971-bc6654cee1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame index to position mapping\n",
    "index_to_pos = {idx: pos for pos, idx in enumerate(df.index)}\n",
    "\n",
    "# Get the positions of the last transaction in each sequence\n",
    "seq_positions = [index_to_pos[idx] for idx in sequence_indices]\n",
    "\n",
    "# Split into train and test sets (80-20 split)\n",
    "(X_seq_train, X_seq_test, \n",
    " seq_pos_train, seq_pos_test,\n",
    " y_train, y_test) = train_test_split(\n",
    "    data_sequences,\n",
    "    seq_positions,\n",
    "    labels,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=labels\n",
    ")\n",
    "\n",
    "# Prepare input dictionaries\n",
    "def create_input_dict(sequences, positions, df):\n",
    "    inputs = {'numerical_seq': sequences}\n",
    "    \n",
    "    # Add categorical inputs\n",
    "    for col in categorical_cols:\n",
    "        inputs[f'{col}_input'] = df.iloc[positions][col].values.reshape(-1, 1)\n",
    "    \n",
    "    # Add binary features\n",
    "    inputs['binary_features'] = df.iloc[positions][binary_cols].values\n",
    "    \n",
    "    return inputs\n",
    "\n",
    "train_data = create_input_dict(X_seq_train, seq_pos_train, df)\n",
    "test_data = create_input_dict(X_seq_test, seq_pos_test, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c638497e-a0a5-4c95-86ff-bb4d0bbe148f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze categorical feature cardinalities\n",
    "categorical_cardinalities = {col: df[col].nunique() for col in categorical_cols}\n",
    "embedding_dims = {\n",
    "    col: min(50, max(2, card // 2 + 1)) \n",
    "    for col, card in categorical_cardinalities.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2060980e-ad06-487b-bcc2-8fc4be9f0e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_deep_rnn_model(sequence_length, numerical_cols, categorical_cols, binary_cols, \n",
    "                         categorical_cardinalities, embedding_dims):\n",
    "    # Numerical sequence input (Deep LSTM path)\n",
    "    numerical_input = Input(shape=(sequence_length, len(numerical_cols)), name='numerical_seq')\n",
    "    \n",
    "    # Deep LSTM layers (3 layers)\n",
    "    lstm_layer1 = LSTM(128, return_sequences=True, name='LSTM_1')(numerical_input)\n",
    "    lstm_layer1 = Dropout(0.2)(lstm_layer1)\n",
    "    \n",
    "    lstm_layer2 = LSTM(128, return_sequences=True, name='LSTM_2')(lstm_layer1)\n",
    "    lstm_layer2 = Dropout(0.2)(lstm_layer2)\n",
    "    \n",
    "    lstm_layer3 = LSTM(128, name='LSTM_3')(lstm_layer2)\n",
    "    lstm_layer3 = Dropout(0.2)(lstm_layer3)\n",
    "    \n",
    "    # Entity embeddings for categorical features\n",
    "    categorical_inputs = []\n",
    "    categorical_embeddings = []\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        input_layer = Input(shape=(1,), name=f'{col}_input')\n",
    "        categorical_inputs.append(input_layer)\n",
    "        \n",
    "        embedding = Embedding(\n",
    "            input_dim=categorical_cardinalities[col] + 1,\n",
    "            output_dim=embedding_dims[col],\n",
    "            name=f'{col}_embedding',\n",
    "            embeddings_regularizer=l2(1e-4)\n",
    "        )(input_layer)\n",
    "        \n",
    "        flattened = Flatten()(embedding)\n",
    "        flattened = Dropout(0.1)(flattened)\n",
    "        categorical_embeddings.append(flattened)\n",
    "    \n",
    "    # Binary features input\n",
    "    binary_input = Input(shape=(len(binary_cols),), name='binary_features')\n",
    "    \n",
    "    # Combine all features\n",
    "    combined = Concatenate()([lstm_layer3] + categorical_embeddings + [binary_input])\n",
    "    \n",
    "    # Feedforward \n",
    "    x = Dense(256, activation='relu')(combined)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    # Output layer\n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(\n",
    "        inputs=[numerical_input] + categorical_inputs + [binary_input],\n",
    "        outputs=output\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b3f46b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['numerical_seq', 'channel_input', 'currency_input', 'device_input', 'payment_method_input', 'category_input', 'country_x_input', 'country_y_input', 'sex_input', 'education_input', 'primary_source_of_income_input', 'binary_features'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fddc2832-c9dd-434a-9041-b61f8c5b580b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5081 - auc: 0.5243 - loss: 0.7487\n",
      "Epoch 1: val_auc improved from -inf to 0.55311, saving model to best_model.keras\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 22ms/step - accuracy: 0.5081 - auc: 0.5243 - loss: 0.7487 - val_accuracy: 0.6812 - val_auc: 0.5531 - val_loss: 0.6584 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5146 - auc: 0.5482 - loss: 0.6906\n",
      "Epoch 2: val_auc improved from 0.55311 to 0.55653, saving model to best_model.keras\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 21ms/step - accuracy: 0.5146 - auc: 0.5482 - loss: 0.6906 - val_accuracy: 0.5104 - val_auc: 0.5565 - val_loss: 0.6922 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4963 - auc: 0.5445 - loss: 0.6934\n",
      "Epoch 3: val_auc did not improve from 0.55653\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 21ms/step - accuracy: 0.4963 - auc: 0.5445 - loss: 0.6934 - val_accuracy: 0.4970 - val_auc: 0.5561 - val_loss: 0.7107 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5135 - auc: 0.5470 - loss: 0.6898\n",
      "Epoch 4: val_auc did not improve from 0.55653\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 21ms/step - accuracy: 0.5135 - auc: 0.5470 - loss: 0.6898 - val_accuracy: 0.5019 - val_auc: 0.5541 - val_loss: 0.7091 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m3123/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4982 - auc: 0.5483 - loss: 0.6910\n",
      "Epoch 5: val_auc improved from 0.55653 to 0.55993, saving model to best_model.keras\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 21ms/step - accuracy: 0.4982 - auc: 0.5483 - loss: 0.6910 - val_accuracy: 0.5184 - val_auc: 0.5599 - val_loss: 0.7079 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4996 - auc: 0.5522 - loss: 0.6916\n",
      "Epoch 6: val_auc improved from 0.55993 to 0.56277, saving model to best_model.keras\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 21ms/step - accuracy: 0.4996 - auc: 0.5522 - loss: 0.6916 - val_accuracy: 0.5231 - val_auc: 0.5628 - val_loss: 0.6798 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m3123/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5228 - auc: 0.5570 - loss: 0.6877\n",
      "Epoch 7: val_auc improved from 0.56277 to 0.56456, saving model to best_model.keras\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 22ms/step - accuracy: 0.5228 - auc: 0.5570 - loss: 0.6877 - val_accuracy: 0.5067 - val_auc: 0.5646 - val_loss: 0.7097 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m3124/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5136 - auc: 0.5604 - loss: 0.6863\n",
      "Epoch 8: val_auc improved from 0.56456 to 0.57155, saving model to best_model.keras\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 22ms/step - accuracy: 0.5136 - auc: 0.5604 - loss: 0.6863 - val_accuracy: 0.6540 - val_auc: 0.5715 - val_loss: 0.6434 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m3124/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5007 - auc: 0.5640 - loss: 0.6871\n",
      "Epoch 9: val_auc improved from 0.57155 to 0.57331, saving model to best_model.keras\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 22ms/step - accuracy: 0.5007 - auc: 0.5640 - loss: 0.6871 - val_accuracy: 0.5199 - val_auc: 0.5733 - val_loss: 0.6914 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5038 - auc: 0.5678 - loss: 0.6864\n",
      "Epoch 10: val_auc improved from 0.57331 to 0.57347, saving model to best_model.keras\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 22ms/step - accuracy: 0.5038 - auc: 0.5678 - loss: 0.6864 - val_accuracy: 0.5037 - val_auc: 0.5735 - val_loss: 0.6781 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m3124/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5013 - auc: 0.5709 - loss: 0.6880\n",
      "Epoch 11: val_auc did not improve from 0.57347\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 22ms/step - accuracy: 0.5013 - auc: 0.5709 - loss: 0.6880 - val_accuracy: 0.4191 - val_auc: 0.5710 - val_loss: 0.7090 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m3123/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.4940 - auc: 0.5699 - loss: 0.6875\n",
      "Epoch 12: val_auc improved from 0.57347 to 0.57366, saving model to best_model.keras\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 22ms/step - accuracy: 0.4940 - auc: 0.5699 - loss: 0.6875 - val_accuracy: 0.4993 - val_auc: 0.5737 - val_loss: 0.6741 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5002 - auc: 0.5687 - loss: 0.6866\n",
      "Epoch 13: val_auc did not improve from 0.57366\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 22ms/step - accuracy: 0.5002 - auc: 0.5687 - loss: 0.6866 - val_accuracy: 0.4840 - val_auc: 0.5734 - val_loss: 0.6975 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m3124/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5084 - auc: 0.5689 - loss: 0.6855\n",
      "Epoch 14: val_auc did not improve from 0.57366\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 22ms/step - accuracy: 0.5084 - auc: 0.5689 - loss: 0.6855 - val_accuracy: 0.5038 - val_auc: 0.5731 - val_loss: 0.6924 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m3124/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5025 - auc: 0.5712 - loss: 0.6863\n",
      "Epoch 15: val_auc did not improve from 0.57366\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 22ms/step - accuracy: 0.5025 - auc: 0.5712 - loss: 0.6863 - val_accuracy: 0.5611 - val_auc: 0.5723 - val_loss: 0.6560 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5019 - auc: 0.5702 - loss: 0.6848\n",
      "Epoch 16: val_auc did not improve from 0.57366\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 22ms/step - accuracy: 0.5019 - auc: 0.5702 - loss: 0.6848 - val_accuracy: 0.4545 - val_auc: 0.5736 - val_loss: 0.7027 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m3123/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5119 - auc: 0.5711 - loss: 0.6849\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 17: val_auc did not improve from 0.57366\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 22ms/step - accuracy: 0.5119 - auc: 0.5711 - loss: 0.6849 - val_accuracy: 0.5275 - val_auc: 0.5730 - val_loss: 0.6578 - learning_rate: 0.0010\n",
      "Epoch 17: early stopping\n",
      "Restoring model weights from the end of the best epoch: 12.\n"
     ]
    }
   ],
   "source": [
    "# Create the model with correct input names\n",
    "model = create_deep_rnn_model(\n",
    "    sequence_length=sequence_length,\n",
    "    numerical_cols=numerical_cols,\n",
    "    categorical_cols=categorical_cols,\n",
    "    binary_cols=binary_cols,\n",
    "    categorical_cardinalities=categorical_cardinalities,\n",
    "    embedding_dims=embedding_dims\n",
    ")\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    y_train,\n",
    "    batch_size=128,\n",
    "    epochs=50,\n",
    "    validation_data=(test_data, y_test),\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_auc',\n",
    "            mode='max',\n",
    "            patience=5,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_auc',\n",
    "            mode='max',\n",
    "            factor=0.1,\n",
    "            patience=5,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            'best_model.keras',\n",
    "            monitor='val_auc',\n",
    "            mode='max',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42cf5295-8e82-410a-b80f-2a1013f37271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 4ms/step - accuracy: 0.4989 - auc: 0.5749 - loss: 0.6742\n",
      "\n",
      "Test Set Evaluation:\n",
      "Loss: 0.6741\n",
      "Accuracy: 0.4993\n",
      "AUC: 0.5737\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 4ms/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.97      0.94     91518\n",
      "           1       0.12      0.04      0.06      8482\n",
      "\n",
      "    accuracy                           0.89    100000\n",
      "   macro avg       0.52      0.51      0.50    100000\n",
      "weighted avg       0.85      0.89      0.87    100000\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[89106  2412]\n",
      " [ 8151   331]]\n"
     ]
    }
   ],
   "source": [
    "test_results = model.evaluate(test_data, y_test)\n",
    "print(\"\\nTest Set Evaluation:\")\n",
    "print(f\"Loss: {test_results[0]:.4f}\")\n",
    "print(f\"Accuracy: {test_results[1]:.4f}\")\n",
    "print(f\"AUC: {test_results[2]:.4f}\")\n",
    "\n",
    "\n",
    "y_pred = model.predict(test_data)\n",
    "y_pred_classes = (y_pred > 0.4).astype(int)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350aa4fc-1d55-48b2-9824-d12e5854e032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f40473cd-b439-41aa-a4dc-c28ab94be233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Evaluation:\n",
      "Accuracy: 0.7103\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.75      0.82     91518\n",
      "           1       0.11      0.33      0.16      8482\n",
      "\n",
      "    accuracy                           0.71    100000\n",
      "   macro avg       0.52      0.54      0.49    100000\n",
      "weighted avg       0.85      0.71      0.77    100000\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[68220 23298]\n",
      " [ 5673  2809]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# y_pred = model.predict(test_data)\n",
    "y_pred_classes = (y_pred > 0.55).astype(int)\n",
    "\n",
    "print(\"\\nModel Evaluation:\")\n",
    "print(\"Accuracy: {:.4f}\".format(accuracy_score(y_test, y_pred_classes)))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da4d3372-c9a3-417e-8ba6-33bb0bad8c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Evaluation:\n",
      "Accuracy: 0.5742\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.58      0.71     91518\n",
      "           1       0.10      0.53      0.17      8482\n",
      "\n",
      "    accuracy                           0.57    100000\n",
      "   macro avg       0.52      0.55      0.44    100000\n",
      "weighted avg       0.86      0.57      0.67    100000\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[52954 38564]\n",
      " [ 4015  4467]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# y_pred = model.predict(test_data)\n",
    "y_pred_classes = (y_pred > 0.525).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "print(\"\\nModel Evaluation:\")\n",
    "print(\"Accuracy: {:.4f}\".format(accuracy_score(y_test, y_pred_classes)))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b74be708-dbae-4fce-bb83-340f097f61f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Evaluation:\n",
      "Accuracy: 0.8420\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.91     91518\n",
      "           1       0.12      0.14      0.13      8482\n",
      "\n",
      "    accuracy                           0.84    100000\n",
      "   macro avg       0.52      0.52      0.52    100000\n",
      "weighted avg       0.85      0.84      0.85    100000\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[83049  8469]\n",
      " [ 7335  1147]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# y_pred = model.predict(test_data)\n",
    "y_pred_classes = (y_pred > 0.575).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "print(\"\\nModel Evaluation:\")\n",
    "print(\"Accuracy: {:.4f}\".format(accuracy_score(y_test, y_pred_classes)))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644f1a9a-9d0f-4c68-88df-dc96cdfe7721",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
